# -*- coding: utf-8 -*-
"""MIMO-RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GNNEuXEcnsJa1cB2zxqOs3ecZKmtuRwy
"""

#!/usr/bin/env python3
"""
Two-stage RL for channel/precoder prediction:

Stage 1: Discrete RL (DQN) predicts W1 (discrete codebook index)
Stage 2: Continuous RL (DDPG) predicts W2 (continuous), then quantizes into WC1 and WC2.

Dependencies:
  pip install gymnasium numpy torch

Run:
  python two_stage_w1_w2_rl.py
"""

from __future__ import annotations

import random
from dataclasses import dataclass
from collections import deque
from typing import Deque, Optional, Tuple, Dict

import numpy as np
import gymnasium as gym
from gymnasium import spaces

import torch
import torch.nn as nn
import torch.optim as optim


# ----------------------------
# Reproducibility
# ----------------------------
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def to_torch(x: np.ndarray, device: torch.device) -> torch.Tensor:
    return torch.as_tensor(x, dtype=torch.float32, device=device)


# ----------------------------
# Helper: simple "channel" + "codebooks"
# ----------------------------
def sample_channel(rng: np.random.Generator, n_rx: int, n_tx: int) -> np.ndarray:
    """Complex Gaussian channel H in C^{n_rx x n_tx} represented as real stack [Re, Im]."""
    re = rng.normal(0, 1, size=(n_rx, n_tx)).astype(np.float32)
    im = rng.normal(0, 1, size=(n_rx, n_tx)).astype(np.float32)
    return np.stack([re, im], axis=0)  # shape (2, n_rx, n_tx)


def normalize_complex_stack(x: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    """Normalize a complex-stacked tensor (2, ..., ...) by its Fro norm."""
    fro = np.sqrt(np.sum(x**2) + eps).astype(np.float32)
    return (x / fro).astype(np.float32)


def make_w1_codebook(rng: np.random.Generator, k: int, n_tx: int, n_streams: int) -> np.ndarray:
    """
    Dummy discrete codebook for W1: k matrices in C^{n_tx x n_streams}, stored as (k, 2, n_tx, n_streams).
    """
    cb = []
    for _ in range(k):
        re = rng.normal(0, 1, size=(n_tx, n_streams)).astype(np.float32)
        im = rng.normal(0, 1, size=(n_tx, n_streams)).astype(np.float32)
        w = np.stack([re, im], axis=0)
        cb.append(normalize_complex_stack(w))
    return np.stack(cb, axis=0)


def make_w2_quant_codebooks(rng: np.random.Generator, k1: int, k2: int, n_tx: int, n_streams: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    Two-stage quantization codebooks for W2:
      WC1 in {0..k1-1} and WC2 in {0..k2-1}
    Stored as complex stacks: (k?, 2, n_tx, n_streams)
    """
    cb1 = make_w1_codebook(rng, k1, n_tx, n_streams)
    cb2 = make_w1_codebook(rng, k2, n_tx, n_streams)
    return cb1, cb2


def quantize_two_stage(w2_cont: np.ndarray, cb1: np.ndarray, cb2: np.ndarray) -> Tuple[int, int, np.ndarray]:
    """
    Two-stage quantization:
      pick i = argmin ||w2 - cb1[i]||_F
      residual = w2 - cb1[i]
      pick j = argmin ||residual - cb2[j]||_F
      reconstruct = cb1[i] + cb2[j]
    """
    # w2_cont shape: (2, n_tx, n_streams)
    dif1 = cb1 - w2_cont[None, ...]
    err1 = np.sum(dif1**2, axis=(1, 2, 3))
    i = int(np.argmin(err1))

    residual = (w2_cont - cb1[i]).astype(np.float32)
    dif2 = cb2 - residual[None, ...]
    err2 = np.sum(dif2**2, axis=(1, 2, 3))
    j = int(np.argmin(err2))

    w2_hat = (cb1[i] + cb2[j]).astype(np.float32)
    return i, j, w2_hat


def effective_snr_proxy(H: np.ndarray, W: np.ndarray) -> float:
    """
    Very rough proxy for link quality. Later consider real PHY (SINR per layer, etc.).
    H: (2,n_rx,n_tx), W: (2,n_tx,n_streams)
    We'll compute ||H * W||_F^2 as a proxy.
    """
    # Convert stack to complex for convenience
    Hc = H[0] + 1j * H[1]  # (n_rx,n_tx)
    Wc = W[0] + 1j * W[1]  # (n_tx,n_streams)
    HW = Hc @ Wc
    val = float(np.real(np.vdot(HW, HW)))  # ||HW||_F^2
    return val


def bler_proxy_from_snr(snr: float) -> float:
    """
    Smooth BLER-ish curve (0..1). Replace with actual BLER lookup.
    """
    # Map snr proxy to BLER; larger snr -> smaller bler
    # Adjust scaling as needed.
    x = (snr - 5.0) / 5.0
    bler = 1.0 / (1.0 + np.exp(3.0 * x))
    return float(np.clip(bler, 0.0, 1.0))


# ----------------------------
# Stage 1 Env: choose W1 (discrete)
# ----------------------------
class W1DecisionEnv(gym.Env):
    """
    One-step environment:
      Observation: channel features (flattened H) (+ optional context)
      Action: discrete index for W1
      Reward: based on link quality using W1 (proxy)
    Returns terminated=True in one step.
    """

    metadata = {"render_modes": []}

    def __init__(self, n_rx=2, n_tx=4, n_streams=2, w1_k=16, seed=0):
        super().__init__()
        self.n_rx, self.n_tx, self.n_streams = n_rx, n_tx, n_streams
        self.w1_k = w1_k
        self.rng = np.random.default_rng(seed)

        self.W1_cb = make_w1_codebook(self.rng, w1_k, n_tx, n_streams)

        # Observation: flattened channel (2*n_rx*n_tx)
        obs_dim = 2 * n_rx * n_tx
        self.observation_space = spaces.Box(low=-5.0, high=5.0, shape=(obs_dim,), dtype=np.float32)
        self.action_space = spaces.Discrete(w1_k)

        self.H = None

    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        if seed is not None:
            self.rng = np.random.default_rng(seed)

        self.H = normalize_complex_stack(sample_channel(self.rng, self.n_rx, self.n_tx))
        obs = self.H.reshape(-1).astype(np.float32)
        return obs, {}

    def step(self, action: int):
        idx = int(action)
        W1 = self.W1_cb[idx]

        snr = effective_snr_proxy(self.H, W1)
        bler = bler_proxy_from_snr(snr)

        # Reward example: encourage low BLER and decent SNR
        # You can make it PMI+BLER-like (your slide that shared with me, mentions coarse reward is currently used). :contentReference[oaicite:1]{index=1}
        reward = (1.0 - bler)

        terminated, truncated = True, False
        obs = self.H.reshape(-1).astype(np.float32)
        info = {"W1_index": idx, "snr_proxy": snr, "bler_proxy": bler}
        return obs, float(reward), terminated, truncated, info


# ----------------------------
# Stage 2 Env: predict W2 continuous, then quantize -> WC1, WC2
# ----------------------------
class W2ControlEnv(gym.Env):
    """
    Multi-step (or one-step) environment for W2 prediction conditioned on chosen W1.

    - reset(options={"W1_index": idx}) sets the context
    - action is continuous: predicts W2 (flattened real/imag)
    - environment quantizes into WC1 and WC2 using two codebooks
    - reward computed using combined precoder W = f(W1, W2_hat)

    For simplicity, this env is one-step:
      action -> quantize -> reward -> done
    """

    metadata = {"render_modes": []}

    def __init__(self, n_rx=2, n_tx=4, n_streams=2, w1_k=16, wc1_k=8, wc2_k=8, seed=1):
        super().__init__()
        self.n_rx, self.n_tx, self.n_streams = n_rx, n_tx, n_streams
        self.w1_k = w1_k
        self.wc1_k, self.wc2_k = wc1_k, wc2_k
        self.rng = np.random.default_rng(seed)

        # Re-create same codebooks (in practice share them / pass in)
        self.W1_cb = make_w1_codebook(self.rng, w1_k, n_tx, n_streams)
        self.WC1_cb, self.WC2_cb = make_w2_quant_codebooks(self.rng, wc1_k, wc2_k, n_tx, n_streams)

        # Observation: flattened H plus one-hot(W1_index)  (context injection)
        h_dim = 2 * n_rx * n_tx
        obs_dim = h_dim + w1_k
        self.observation_space = spaces.Box(low=-10.0, high=10.0, shape=(obs_dim,), dtype=np.float32)

        # Action: continuous W2 prediction as (2, n_tx, n_streams) flattened
        act_dim = 2 * n_tx * n_streams
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(act_dim,), dtype=np.float32)

        self.H = None
        self.W1_index = 0

    def _obs(self) -> np.ndarray:
        h = self.H.reshape(-1).astype(np.float32)
        onehot = np.zeros(self.w1_k, dtype=np.float32)
        onehot[self.W1_index] = 1.0
        return np.concatenate([h, onehot], axis=0)

    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        if seed is not None:
            self.rng = np.random.default_rng(seed)

        options = options or {}
        self.W1_index = int(options.get("W1_index", self.rng.integers(0, self.w1_k)))

        self.H = normalize_complex_stack(sample_channel(self.rng, self.n_rx, self.n_tx))
        return self._obs(), {"W1_index": self.W1_index}

    def step(self, action: np.ndarray):
        # Map action to W2 continuous tensor
        a = np.asarray(action, dtype=np.float32).reshape(2, self.n_tx, self.n_streams)
        w2_cont = normalize_complex_stack(a)

        # Quantize into WC1, WC2, reconstruct W2_hat
        wc1_idx, wc2_idx, w2_hat = quantize_two_stage(w2_cont, self.WC1_cb, self.WC2_cb)

        # Combine W1 and W2_hat (placeholder combination; replace with your real structure)
        # For example, W = W1 + alpha*W2_hat or something reflecting your decomposition.
        W1 = self.W1_cb[self.W1_index]
        W = normalize_complex_stack(W1 + 0.5 * w2_hat)

        snr = effective_snr_proxy(self.H, W)
        bler = bler_proxy_from_snr(snr)

        # Reward example (replace with PMI+BLER or throughput):
        reward = (1.0 - bler)

        terminated, truncated = True, False
        info = {
            "W1_index": self.W1_index,
            "WC1_index": wc1_idx,
            "WC2_index": wc2_idx,
            "snr_proxy": snr,
            "bler_proxy": bler,
        }
        return self._obs(), float(reward), terminated, truncated, info


# ----------------------------
# Replay Buffer
# ----------------------------
@dataclass
class Transition:
    s: np.ndarray
    a: np.ndarray
    r: float
    s2: np.ndarray
    done: float


class ReplayBuffer:
    def __init__(self, capacity: int, seed: int = 0):
        self.buf: Deque[Transition] = deque(maxlen=capacity)
        self.rng = np.random.default_rng(seed)

    def __len__(self):
        return len(self.buf)

    def push(self, s, a, r, s2, done):
        self.buf.append(Transition(s, a, r, s2, done))

    def sample(self, batch_size: int):
        idx = self.rng.choice(len(self.buf), size=batch_size, replace=False)
        batch = [self.buf[i] for i in idx]
        s = np.stack([b.s for b in batch], axis=0)
        a = np.stack([b.a for b in batch], axis=0)
        r = np.array([b.r for b in batch], dtype=np.float32)[:, None]
        s2 = np.stack([b.s2 for b in batch], axis=0)
        done = np.array([b.done for b in batch], dtype=np.float32)[:, None]
        return s, a, r, s2, done


# ----------------------------
# Stage 1: DQN
# ----------------------------
class QNet(nn.Module):
    def __init__(self, obs_dim: int, n_actions: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 256), nn.ReLU(),
            nn.Linear(256, 256), nn.ReLU(),
            nn.Linear(256, n_actions),
        )

    def forward(self, x):
        return self.net(x)


def train_dqn(
    env: gym.Env,
    device: torch.device,
    seed: int = 0,
    steps: int = 30_000,
    gamma: float = 0.99,
    lr: float = 1e-3,
    batch_size: int = 128,
    buffer_size: int = 200_000,
    start_learning: int = 2_000,
    target_update: int = 500,
    eps_start: float = 1.0,
    eps_end: float = 0.05,
    eps_decay_steps: int = 20_000,
) -> QNet:
    set_seed(seed)

    obs_dim = env.observation_space.shape[0]
    n_actions = env.action_space.n

    q = QNet(obs_dim, n_actions).to(device)
    q_t = QNet(obs_dim, n_actions).to(device)
    q_t.load_state_dict(q.state_dict())

    opt = optim.Adam(q.parameters(), lr=lr)
    buf = ReplayBuffer(buffer_size, seed=seed)

    obs, _ = env.reset(seed=seed)
    ep_ret = 0.0
    rets = []

    def eps(step: int) -> float:
        frac = min(step / eps_decay_steps, 1.0)
        return eps_start + frac * (eps_end - eps_start)

    for step in range(1, steps + 1):
        e = eps(step)
        if random.random() < e:
            act = env.action_space.sample()
        else:
            with torch.no_grad():
                act = int(torch.argmax(q(to_torch(obs[None, :], device)), dim=1).item())

        obs2, r, term, trunc, info = env.step(act)
        done = float(term or trunc)
        ep_ret += float(r)

        buf.push(obs, np.array([act], dtype=np.float32), float(r), obs2, done)

        if done:
            rets.append(ep_ret)
            ep_ret = 0.0
            obs, _ = env.reset()
        else:
            obs = obs2

        if len(buf) >= start_learning:
            s, a, r_b, s2, d_b = buf.sample(batch_size)
            s_t = to_torch(s, device)
            a_t = torch.as_tensor(a, dtype=torch.long, device=device)
            r_t = to_torch(r_b, device)
            s2_t = to_torch(s2, device)
            d_t = to_torch(d_b, device)

            q_sa = q(s_t).gather(1, a_t)
            with torch.no_grad():
                y = r_t + gamma * (1.0 - d_t) * q_t(s2_t).max(dim=1, keepdim=True).values

            loss = (q_sa - y).pow(2).mean()

            opt.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(q.parameters(), 10.0)
            opt.step()

            if step % target_update == 0:
                q_t.load_state_dict(q.state_dict())

        if step % 3000 == 0 and rets:
            print(f"[DQN] step={step} eps={e:.3f} avg_ret(last100)={np.mean(rets[-100:]):.3f}")

    return q


@torch.no_grad()
def dqn_select(q: QNet, obs: np.ndarray, device: torch.device) -> int:
    return int(torch.argmax(q(to_torch(obs[None, :], device)), dim=1).item())


# ----------------------------
# Stage 2: DDPG
# ----------------------------
class Actor(nn.Module):
    def __init__(self, obs_dim: int, act_dim: int, act_limit: float):
        super().__init__()
        self.act_limit = act_limit
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 512), nn.ReLU(),
            nn.Linear(512, 512), nn.ReLU(),
            nn.Linear(512, act_dim),
            nn.Tanh(),
        )

    def forward(self, x):
        return self.act_limit * self.net(x)


class Critic(nn.Module):
    def __init__(self, obs_dim: int, act_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim + act_dim, 512), nn.ReLU(),
            nn.Linear(512, 512), nn.ReLU(),
            nn.Linear(512, 1),
        )

    def forward(self, s, a):
        return self.net(torch.cat([s, a], dim=-1))


@torch.no_grad()
def polyak(src: nn.Module, tgt: nn.Module, tau: float):
    for p, pt in zip(src.parameters(), tgt.parameters()):
        pt.data.mul_(1 - tau).add_(tau * p.data)


def train_ddpg(
    env: gym.Env,
    device: torch.device,
    seed: int = 1,
    steps: int = 80_000,
    gamma: float = 0.99,
    lr_a: float = 1e-4,
    lr_c: float = 1e-4,
    batch_size: int = 256,
    buffer_size: int = 400_000,
    start_learning: int = 5_000,
    tau: float = 0.005,
    act_noise: float = 0.15,
) -> Actor:
    set_seed(seed)

    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.shape[0]
    act_limit = float(env.action_space.high[0])

    actor = Actor(obs_dim, act_dim, act_limit).to(device)
    critic = Critic(obs_dim, act_dim).to(device)
    actor_t = Actor(obs_dim, act_dim, act_limit).to(device)
    critic_t = Critic(obs_dim, act_dim).to(device)
    actor_t.load_state_dict(actor.state_dict())
    critic_t.load_state_dict(critic.state_dict())

    opt_a = optim.Adam(actor.parameters(), lr=lr_a)
    opt_c = optim.Adam(critic.parameters(), lr=lr_c)

    buf = ReplayBuffer(buffer_size, seed=seed)

    obs, _ = env.reset(seed=seed)
    ep_ret = 0.0
    rets = []

    for step in range(1, steps + 1):
        with torch.no_grad():
            a = actor(to_torch(obs[None, :], device)).cpu().numpy()[0]
        a = a + np.random.normal(0.0, act_noise, size=a.shape).astype(np.float32)
        a = np.clip(a, -act_limit, act_limit)

        obs2, r, term, trunc, info = env.step(a)
        done = float(term or trunc)
        ep_ret += float(r)

        buf.push(obs, a.astype(np.float32), float(r), obs2, done)

        if done:
            rets.append(ep_ret)
            ep_ret = 0.0
            obs, _ = env.reset()
        else:
            obs = obs2

        if len(buf) >= start_learning:
            s, a_b, r_b, s2, d_b = buf.sample(batch_size)
            s_t = to_torch(s, device)
            a_t = to_torch(a_b, device)
            r_t = to_torch(r_b, device)
            s2_t = to_torch(s2, device)
            d_t = to_torch(d_b, device)

            with torch.no_grad():
                a2 = actor_t(s2_t)
                q2 = critic_t(s2_t, a2)
                y = r_t + gamma * (1.0 - d_t) * q2

            qv = critic(s_t, a_t)
            loss_c = (qv - y).pow(2).mean()
            opt_c.zero_grad()
            loss_c.backward()
            nn.utils.clip_grad_norm_(critic.parameters(), 10.0)
            opt_c.step()

            loss_a = -critic(s_t, actor(s_t)).mean()
            opt_a.zero_grad()
            loss_a.backward()
            nn.utils.clip_grad_norm_(actor.parameters(), 10.0)
            opt_a.step()

            polyak(actor, actor_t, tau)
            polyak(critic, critic_t, tau)

        if step % 5000 == 0 and rets:
            print(f"[DDPG] step={step} avg_ret(last50)={np.mean(rets[-50:]):.3f}")

    return actor


# ----------------------------
# End-to-end evaluation: DQN -> DDPG
# ----------------------------
def evaluate_pipeline(
    env1: W1DecisionEnv,
    env2: W2ControlEnv,
    q: QNet,
    actor: Actor,
    device: torch.device,
    episodes: int = 200,
) -> Dict[str, float]:
    r1s, r2s, blers = [], [], []
    for _ in range(episodes):
        o1, _ = env1.reset()
        w1_idx = dqn_select(q, o1, device)
        _, r1, *_ , info1 = env1.step(w1_idx)

        o2, _ = env2.reset(options={"W1_index": w1_idx})
        with torch.no_grad():
            a = actor(to_torch(o2[None, :], device)).cpu().numpy()[0]
        _, r2, *_ , info2 = env2.step(a)

        r1s.append(float(r1))
        r2s.append(float(r2))
        blers.append(float(info2["bler_proxy"]))

    return {
        "episodes": float(episodes),
        "avg_stage1_reward": float(np.mean(r1s)),
        "avg_stage2_reward": float(np.mean(r2s)),
        "avg_bler_proxy": float(np.mean(blers)),
    }


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device:", device)

    # Problem sizes (edit these to match your MIMO setup)
    n_rx, n_tx, n_streams = 2, 4, 2

    # Codebook sizes (edit these)
    W1_K = 16      # discrete W1 choices
    WC1_K = 8      # quant stage 1
    WC2_K = 8      # quant stage 2

    # --- Stage 1 env & train DQN
    env1 = W1DecisionEnv(n_rx=n_rx, n_tx=n_tx, n_streams=n_streams, w1_k=W1_K, seed=0)
    q = train_dqn(env1, device=device, seed=0, steps=30_000)

    # --- Stage 2 env & train DDPG (it will sample W1_index at reset during training)
    env2 = W2ControlEnv(n_rx=n_rx, n_tx=n_tx, n_streams=n_streams, w1_k=W1_K, wc1_k=WC1_K, wc2_k=WC2_K, seed=1)
    actor = train_ddpg(env2, device=device, seed=1, steps=80_000)

    # --- Evaluate cascade
    metrics = evaluate_pipeline(env1, env2, q, actor, device, episodes=500)
    print("\nPipeline metrics:")
    for k, v in metrics.items():
        print(f"  {k}: {v:.4f}")

    # Show one sample roll
    o1, _ = env1.reset()
    w1_idx = dqn_select(q, o1, device)
    _, r1, *_ , info1 = env1.step(w1_idx)

    o2, info2r = env2.reset(options={"W1_index": w1_idx})
    with torch.no_grad():
        a = actor(to_torch(o2[None, :], device)).cpu().numpy()[0]
    _, r2, *_ , info2 = env2.step(a)

    print("\nOne episode example:")
    print(f"  Stage1: W1_index={w1_idx}, r1={r1:.3f}, bler1~{info1['bler_proxy']:.3f}")
    print(f"  Stage2: WC1={info2['WC1_index']}, WC2={info2['WC2_index']}, r2={r2:.3f}, bler2~{info2['bler_proxy']:.3f}")


if __name__ == "__main__":
    main()